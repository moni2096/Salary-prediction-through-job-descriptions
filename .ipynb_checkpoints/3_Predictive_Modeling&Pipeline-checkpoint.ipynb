{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LaHgF64JLpyM"
   },
   "source": [
    "#### In this notebook we will be automating the entire process that we did in the data modeling notebook. We will be writing the code in Object Oriented Paradigm. The 3 algorithms that we tested our data in the data modeling phase was:\n",
    " - Linear Regression\n",
    " - Random Forest Regressor\n",
    " - Gradient Boosting Resgressor\n",
    "#### We will later automate the pipleine and also deploy the solutions after selecting the model that performs better than the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oY-5VgxjLpyQ"
   },
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GYiukLKsLpyU"
   },
   "outputs": [],
   "source": [
    "# Function to read the csv file and store it into the dataframe\n",
    "def read_csv(file):\n",
    "    return pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-OeX8CeLpyY"
   },
   "outputs": [],
   "source": [
    "# Function to clean the data. This is the same function that we used in data modeling noteobook\n",
    "def clean_df(raw_df):\n",
    "    # Dropping jobId column if they are duplicates and selecting only those salaries that are greater than zero\n",
    "    \n",
    "    clean_df = raw_df.drop_duplicates(subset='jobId')\n",
    "    clean_df = clean_df[clean_df.salary>0]\n",
    "    return clean_df\n",
    "\n",
    "# Function to perform inner join two dataframe based on any features \n",
    "\n",
    "def consolidate_data(df1, df2, key=None, left_index=False, right_index=False):\n",
    "    # Perfroming inner join based on the key passed in the data frame\n",
    "    \n",
    "    return pd.merge(left=df1, right=df2, how='inner', on=key, left_index=left_index, right_index=right_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6aN9s7XLpyc"
   },
   "outputs": [],
   "source": [
    "# Function to perform one hot encoding on the dataframe\n",
    "def one_hot_encode_df(df, cat_vars=None, num_vars=None):\n",
    "    cat_df = pd.get_dummies(df[cat_vars])\n",
    "    num_df = df[num_vars].apply(pd.to_numeric)\n",
    "    return pd.concat([cat_df, num_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7wHR05ELpyf"
   },
   "outputs": [],
   "source": [
    "# Function to return the target dataframe\n",
    "def get_target_df(df, target):\n",
    "    return df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ph9BafWOLpyj"
   },
   "outputs": [],
   "source": [
    "# Function to train the models that we specified above\n",
    "def train_models(model, feature_df, target_df, num_procs, mean_mse, cv_std):\n",
    "    neg_mse = cross_val_score(model, feature_df, target_df, cv=2, n_jobs=num_procs, scoring='neg_mean_squared_error')\n",
    "    mean_mse[model] = -1.0*np.mean(neg_mse)\n",
    "    cv_std[model] = np.std(neg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CEbjnttrLpym"
   },
   "outputs": [],
   "source": [
    "# Function to print the summary of model\n",
    "def print_summary_models(model, mean_mse, cv_std):\n",
    "    print('\\nModel:\\n', model)\n",
    "    print('Average MSE:\\n', mean_mse[model])\n",
    "    print('Standard deviation during CV:\\n', cv_std[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07Nfi2eqLpyp"
   },
   "outputs": [],
   "source": [
    "# Function to save the results(predictions) and also to plot the feature importance into the model\n",
    "def save_results(model, mean_mse, predictions, feature_importances):\n",
    "    with open('model.txt', 'w') as file:\n",
    "        file.write(str(model))\n",
    "    feature_importances.to_csv('feature_importances.csv') \n",
    "    np.savetxt('predictions.csv', predictions, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANFSe-f0Lpys"
   },
   "outputs": [],
   "source": [
    "# Definining train data\n",
    "train_feature_file = 'data/train_features.csv'\n",
    "train_target_file = 'data/train_salaries.csv'\n",
    "test_feature_file = 'data/test_features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwSX3oS9Lpyu"
   },
   "outputs": [],
   "source": [
    "# Defining catergorical, numerical and target variables\n",
    "categorical_vars = ['companyId', 'jobType', 'degree', 'major', 'industry']\n",
    "numeric_vars = ['yearsExperience', 'milesFromMetropolis']\n",
    "target_var = 'salary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "_S6VeKeiLpyx",
    "outputId": "9bdfd18c-38f1-4eda-cfaf-3d8b1c629a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Encoding data\n"
     ]
    }
   ],
   "source": [
    "# Loading the train data\n",
    "print(\"Reading data\")\n",
    "feature_df = read_csv(train_feature_file)\n",
    "target_df = read_csv(train_target_file)\n",
    "test_df = read_csv(test_feature_file)\n",
    "\n",
    "# Merging the train data and target data\n",
    "train_df = consolidate_data(feature_df, target_df, key='jobId')\n",
    "\n",
    "# Cleaning the data\n",
    "clean_train_df = shuffle(clean_df(train_df)).reset_index()\n",
    "\n",
    "# Performing one hot encoding on the training data\n",
    "print(\"Encoding data\")\n",
    "feature_df = one_hot_encode_df(clean_train_df, cat_vars=categorical_vars, num_vars=numeric_vars)\n",
    "test_df = one_hot_encode_df(test_df, cat_vars=categorical_vars, num_vars=numeric_vars)\n",
    "\n",
    "# Get target df\n",
    "target_df = get_target_df(clean_train_df, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDpr5lQiLpy3"
   },
   "outputs": [],
   "source": [
    "# Initializing model list and disctionaris for mean mas enad cross validation standard deviation\n",
    "models = []\n",
    "mean_mse = {}\n",
    "cv_std = {}\n",
    "res = {}\n",
    "\n",
    "# Define number of processes to run in parallel\n",
    "num_procs = 2\n",
    "\n",
    "# Define shared model paremeter\n",
    "verbose_lvl = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YiOOCT0CLpy_"
   },
   "source": [
    "#### We already did hyperperameter tuning of each model in Data Modeling notebook. So we will simply create model objects and train them.\n",
    " - We will store the mean_mse in dictionary\n",
    " - We will perform cross-validation using k-fold cross validation \n",
    " - We will store the result in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1POUfZLELpzA"
   },
   "outputs": [],
   "source": [
    "# Create model objects\n",
    "lr = LinearRegression()\n",
    "rf = RandomForestRegressor(n_estimators=180, n_jobs=num_procs, max_depth=30, min_samples_split=60,\\\n",
    "                           max_features=30, verbose=verbose_lvl)\n",
    "gbm = GradientBoostingRegressor(n_estimators=180, max_depth=10, loss='ls', verbose=verbose_lvl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "1oWAS-NQLpzD",
    "outputId": "ac78b209-3829-4940-86b9-3473044873f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model:\n",
      " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
      "Average MSE:\n",
      " 384.45520244056524\n",
      "Standard deviation during CV:\n",
      " 0.23037442568974598\n"
     ]
    }
   ],
   "source": [
    "# Training Linear Regression model, Performing Cross-Validation and printing summary\n",
    "train_models(lr, feature_df, target_df, num_procs, mean_mse, cv_std)\n",
    "print_summary_models(lr, mean_mse, cv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wz_ONL0ELpzG"
   },
   "outputs": [],
   "source": [
    "# Making pipeline for model\n",
    "lr_std_pca = make_pipeline(StandardScaler(), PCA(), LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "ZaXyFi46LpzJ",
    "outputId": "34783433-54ed-476a-bbce-f73bc7b6de15"
   },
   "outputs": [],
   "source": [
    "# Training Random Forest model, Performing Cross-Validation and printing summary\n",
    "train_models(rf, feature_df, target_df, num_procs, mean_mse, cv_std)\n",
    "print_summary_models(rf, mean_mse, cv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7u8JoQKLpzN"
   },
   "outputs": [],
   "source": [
    "# Training Gradient Boosting model, Performing Cross-Validation and printing summary\n",
    "train_models(gbm, feature_df, target_df, num_procs, mean_mse, cv_std)\n",
    "print_summary_models(gbm, mean_mse, cv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9j8ffPELpzQ"
   },
   "outputs": [],
   "source": [
    "# Training Polynomial Fature Model, Perfoming Cross-Validation and printing Summary\n",
    "p = PolynomialFeatures(2)\n",
    "\n",
    "#Fitting object to training/testing data\n",
    "x_train_p = p.fit_transform(clean_train_df)\n",
    "x_test_p = p.fit_transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpYeLVCVOuQj"
   },
   "outputs": [],
   "source": [
    "poly = LinearRegression()\n",
    "poly.fit(x_train_p, target_df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "3_Predictive_Modeling&Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
