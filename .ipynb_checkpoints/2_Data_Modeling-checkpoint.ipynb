{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After the preprocessing step we will be eastablishing the baseline for our model. Also, we know that our problem can be best solved by Regression methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#__author__ = Monish Khambhati\n",
    "#__email__ = monish.khambhati@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the train data\n",
    "train_df = pd.read_csv('data/train_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the target features data\n",
    "target_df = pd.read_csv('data/train_salaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the test features data\n",
    "test_df = pd.read_csv('data/test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the train and target dataframe\n",
    "train_df = pd.merge(left = train_df,right = target_df,how='inner', on='jobId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the EDA conclusion we know that **jobId** is not related to the target variables. So, I will start by dropping those fetaures from train and test data. And also choosing the salaries which are greater than 0 in **train_df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the dataframe\n",
    "def clean_data(raw_df):\n",
    "    '''remove rows that contain salary <= 0 or duplicate job IDs'''\n",
    "    clean_df = raw_df.drop_duplicates(subset='jobId')\n",
    "    clean_df = clean_df[clean_df.salary>0]\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = clean_data(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop('jobId', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding data to our model we will be writing a function to perform **One Hot Encoding**.\n",
    " - One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_df(df, cat_vars=None, num_vars=None):\n",
    "    '''performs one-hot encoding on all categorical variables and combines result with continous variables'''\n",
    "    cat_df = pd.get_dummies(df[cat_vars])\n",
    "    num_df = df[num_vars].apply(pd.to_numeric)\n",
    "    return pd.concat([cat_df, num_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining categorical variable and numerical variables\n",
    "categorical_vars = ['companyId','jobType', 'degree', 'major', 'industry']\n",
    "numerical_vars = ['yearsExperience', 'milesFromMetropolis']\n",
    "target_var = 'salary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = one_hot_encode_df(train_df,cat_vars=categorical_vars, num_vars=numerical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>companyId_COMP0</th>\n",
       "      <th>companyId_COMP1</th>\n",
       "      <th>companyId_COMP10</th>\n",
       "      <th>companyId_COMP11</th>\n",
       "      <th>companyId_COMP12</th>\n",
       "      <th>companyId_COMP13</th>\n",
       "      <th>companyId_COMP14</th>\n",
       "      <th>companyId_COMP15</th>\n",
       "      <th>companyId_COMP16</th>\n",
       "      <th>companyId_COMP17</th>\n",
       "      <th>...</th>\n",
       "      <th>major_PHYSICS</th>\n",
       "      <th>industry_AUTO</th>\n",
       "      <th>industry_EDUCATION</th>\n",
       "      <th>industry_FINANCE</th>\n",
       "      <th>industry_HEALTH</th>\n",
       "      <th>industry_OIL</th>\n",
       "      <th>industry_SERVICE</th>\n",
       "      <th>industry_WEB</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>milesFromMetropolis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   companyId_COMP0  companyId_COMP1  companyId_COMP10  companyId_COMP11  \\\n",
       "0                0                0                 0                 0   \n",
       "1                0                0                 0                 0   \n",
       "2                0                0                 0                 0   \n",
       "3                0                0                 0                 0   \n",
       "4                0                0                 0                 0   \n",
       "\n",
       "   companyId_COMP12  companyId_COMP13  companyId_COMP14  companyId_COMP15  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   companyId_COMP16  companyId_COMP17         ...           major_PHYSICS  \\\n",
       "0                 0                 0         ...                       0   \n",
       "1                 0                 0         ...                       0   \n",
       "2                 0                 0         ...                       1   \n",
       "3                 0                 0         ...                       0   \n",
       "4                 0                 0         ...                       1   \n",
       "\n",
       "   industry_AUTO  industry_EDUCATION  industry_FINANCE  industry_HEALTH  \\\n",
       "0              0                   0                 0                1   \n",
       "1              0                   0                 0                0   \n",
       "2              0                   0                 0                1   \n",
       "3              1                   0                 0                0   \n",
       "4              0                   0                 1                0   \n",
       "\n",
       "   industry_OIL  industry_SERVICE  industry_WEB  yearsExperience  \\\n",
       "0             0                 0             0               10   \n",
       "1             0                 0             1                3   \n",
       "2             0                 0             0               10   \n",
       "3             0                 0             0                8   \n",
       "4             0                 0             0                8   \n",
       "\n",
       "   milesFromMetropolis  \n",
       "0                   83  \n",
       "1                   73  \n",
       "2                   38  \n",
       "3                   17  \n",
       "4                   16  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into training and validation set and establishing the baseline\n",
    " - I will be using sklearn to split the data into training and validation set into 80:20 feataure\n",
    " - Also, the metric that we will choose in this case is Mean Squared Error(MSE). Mean squared error measures the average of the squares of errors, i.e, the difference between actual value (y) and the estimated value (ŷ).\n",
    " - Baseline model would be a simple linear regession model and we will be using it to hypothesize solutions based on the results of the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting target variable from train_salaries which are greater than 0\n",
    "target_df = clean_data(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = target_df.drop('jobId',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and validation set\n",
    "X_train_data, X_test_data, y_train_data, y_test_data = train_test_split(train_df, target_df, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = one_hot_encode_df(test_df,cat_vars=categorical_vars, num_vars=numerical_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 predictied salaries:  [[157.76965332]\n",
      " [ 62.42608643]\n",
      " [108.85063171]\n",
      " [112.38508606]\n",
      " [ 92.39764404]]\n"
     ]
    }
   ],
   "source": [
    "# Creating a linear regression object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fitting the model with the train data\n",
    "lr.fit(X_train_data, y_train_data)\n",
    "\n",
    "# Predicting the model on the validation data\n",
    "y_predict = lr.predict(X_test_data)\n",
    "print(\"The first 5 predictied salaries: \", y_predict[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating our model using validation set by calculating mean squared error\n",
    "mse = mean_squared_error(y_test_data, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384.8722111900365\n"
     ]
    }
   ],
   "source": [
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The k-cross validation accuracy is:  (0.7444241030428751, 0.002713188048878918)\n"
     ]
    }
   ],
   "source": [
    "#Prediction accuracy using k fold cross validation\n",
    "Rcross = cross_val_score(lr, y_test_data, y_predict, cv = 5)\n",
    "\n",
    "print(\"The k-cross validation accuracy is: \", (Rcross.mean(), Rcross.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesize a solution\n",
    "#### On the baseline simple regression model the MSE is 384.87. We would try other techniques and also do feature engineering and hyperparameter tuning to imrove model on test set. \n",
    " - The models we will be using to achieve better accuracy are:\n",
    "     - Random Forest Regressor\n",
    "     - Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 60, max_depth = 25, \n",
    "                           min_samples_split = 20, n_jobs = 2, \n",
    "                           max_features = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
       "           max_features=30, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=60, n_jobs=2, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the object to training data\n",
    "rf.fit(X_train_data, y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7535437432666752"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the model accuracy on test set\n",
    "rf.score(X_test_data, y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 Predictions on test set:  [155.15944532  61.24348629  96.32195964 102.75388373  93.19703172]\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on test data\n",
    "y_predict_rf = rf.predict(X_test_data)\n",
    "print(\"First 5 Predictions on test set: \", y_predict_rf[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371.17114569262134"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating mean squared error on test set\n",
    "mean_squared_error(y_test_data, y_predict_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Gradient Boosting Regressor object\n",
    "gd = GradientBoostingRegressor(n_estimators = 60, max_depth = 5,loss = 'ls', verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1349.8934           12.17m\n",
      "         2        1228.8101           10.40m\n",
      "         3        1128.7405            9.54m\n",
      "         4        1044.5412            9.30m\n",
      "         5         973.8131            9.03m\n",
      "         6         912.9784            8.82m\n",
      "         7         860.9562            8.46m\n",
      "         8         813.5527            8.22m\n",
      "         9         771.8185            8.03m\n",
      "        10         735.6256            7.83m\n",
      "        11         702.8820            7.65m\n",
      "        12         675.2100            7.44m\n",
      "        13         647.1429            7.24m\n",
      "        14         622.9292            7.07m\n",
      "        15         602.7250            6.89m\n",
      "        16         582.9249            6.73m\n",
      "        17         564.0264            6.55m\n",
      "        18         547.9513            6.38m\n",
      "        19         533.2990            6.21m\n",
      "        20         520.3717            6.05m\n",
      "        21         509.0961            5.89m\n",
      "        22         497.0678            5.71m\n",
      "        23         486.3063            5.53m\n",
      "        24         477.7897            5.37m\n",
      "        25         467.8144            5.20m\n",
      "        26         460.7847            5.05m\n",
      "        27         453.6967            4.90m\n",
      "        28         447.8704            4.76m\n",
      "        29         440.7992            4.60m\n",
      "        30         435.6470            4.46m\n",
      "        31         430.0027            4.31m\n",
      "        32         425.5297            4.16m\n",
      "        33         420.3855            4.01m\n",
      "        34         416.0204            3.86m\n",
      "        35         412.2877            3.72m\n",
      "        36         409.2387            3.57m\n",
      "        37         406.3499            3.41m\n",
      "        38         403.0149            3.26m\n",
      "        39         399.9628            3.10m\n",
      "        40         396.8970            2.94m\n",
      "        41         394.6281            2.79m\n",
      "        42         392.2688            2.64m\n",
      "        43         390.4437            2.50m\n",
      "        44         388.5167            2.35m\n",
      "        45         385.9674            2.20m\n",
      "        46         384.5229            2.05m\n",
      "        47         383.0007            1.91m\n",
      "        48         380.9945            1.76m\n",
      "        49         379.6684            1.61m\n",
      "        50         378.4609            1.46m\n",
      "        51         377.4510            1.32m\n",
      "        52         376.4239            1.17m\n",
      "        53         375.2437            1.02m\n",
      "        54         374.3777           52.75s\n",
      "        55         373.1621           43.87s\n",
      "        56         372.3821           35.07s\n",
      "        57         371.6315           26.27s\n",
      "        58         370.7802           17.48s\n",
      "        59         370.0914            8.74s\n",
      "        60         369.4651            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=5, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=60, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=5, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting object to data\n",
    "gd.fit(X_train_data, y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_gd = gd.predict(X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371.70423609623913"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test_data, y_predict_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_data(df1, df2, key=None, left_index=False, right_index=False):\n",
    "    '''perform inner join to return only records that are present in both dataframes'''\n",
    "    return pd.merge(left=df1, right=df2, how='inner', on=key, left_index=left_index, right_index=right_index)\n",
    "\n",
    "def get_target_df(df, target):\n",
    "    '''returns target dataframe'''\n",
    "    return df[target]\n",
    "\n",
    "def train_model(model, feature_df, target_df, num_procs, mean_mse, cv_std):\n",
    "    neg_mse = cross_val_score(model, feature_df, target_df, cv=2, n_jobs=num_procs, scoring='neg_mean_squared_error')\n",
    "    mean_mse[model] = -1.0*np.mean(neg_mse)\n",
    "    cv_std[model] = np.std(neg_mse)\n",
    "\n",
    "def print_summary(model, mean_mse, cv_std):\n",
    "    print('\\nModel:\\n', model)\n",
    "    print('Average MSE:\\n', mean_mse[model])\n",
    "    print('Standard deviation during CV:\\n', cv_std[model])\n",
    "\n",
    "def save_results(model, mean_mse, predictions, feature_importances):\n",
    "    '''saves model, model summary, feature importances, and predictions'''\n",
    "    with open('model.txt', 'w') as file:\n",
    "        file.write(str(model))\n",
    "    feature_importances.to_csv('feature_importances.csv') \n",
    "    np.savetxt('predictions.csv', predictions, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model lists and scripts\n",
    "models = []\n",
    "mean_mse = {}\n",
    "cv_std = {}\n",
    "res= {}\n",
    "\n",
    "# number of processes to run in parellel\n",
    "num_procs = 2\n",
    "\n",
    "#shared model paramaters\n",
    "verbose_lvl = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create models -- hyperparameter tuning already done by hand for each mode\n",
    "\n",
    "lr_std_pca = make_pipeline(StandardScaler(), PCA(), LinearRegression())\n",
    "rf = RandomForestRegressor(n_estimators=150, n_jobs=num_procs, max_depth=25, min_samples_split=60, \\\n",
    "                           max_features=30, verbose=verbose_lvl)\n",
    "gbm = GradientBoostingRegressor(n_estimators=150, max_depth=5, loss='ls', verbose=verbose_lvl)\n",
    "                      \n",
    "models.extend([lr, lr_std_pca, rf, gbm])\n",
    "\n",
    "#parallel cross-validate models, using MSE as evaluation metric, and print summaries\n",
    "print(\"Beginning cross validation\")\n",
    "for model in models:\n",
    "    train_model(model, train_df, target_df, num_procs, mean_mse, cv_std)\n",
    "    print_summary(model, mean_mse, cv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
